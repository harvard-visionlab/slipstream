{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NamedCopies: Multi-View Pipelines from a Single Decode\n",
    "\n",
    "`NamedCopies` duplicates a single decoded image batch into a named dict, bridging\n",
    "single-output decoders (like `DecodeResizeCrop`) with `MultiCropPipeline` which\n",
    "applies separate transforms per view.\n",
    "\n",
    "**Use case**: You want the same center crop, but with different downstream\n",
    "transforms (e.g., different zoom levels, one augmented and one clean, etc.).\n",
    "\n",
    "```\n",
    "JPEG bytes\n",
    "  │\n",
    "  └── DecodeResizeCrop(256, 224)     ← decode once\n",
    "       │\n",
    "       └── NamedCopies(['v1', 'v2'])  ← deep-copy into named dict\n",
    "            │\n",
    "            └── MultiCropPipeline({    ← per-view transforms\n",
    "                 'v1': [zoom=1.0],\n",
    "                 'v2': [zoom=0.5],\n",
    "               })\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LITDATA_VAL_PATH = \"s3://visionlab-datasets/imagenet1k/pre-processed/s256-l512-jpgbytes-q100-streaming/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from slipstream import (\n",
    "    SlipstreamDataset,\n",
    "    SlipstreamLoader,\n",
    "    DecodeResizeCrop,\n",
    "    DecodeCenterCrop,\n",
    "    MultiCropPipeline,\n",
    "    NamedCopies,\n",
    "    ToTorchImage,\n",
    "    Normalize,\n",
    "    IMAGENET_MEAN,\n",
    "    IMAGENET_STD,\n",
    ")\n",
    "from slipstream.transforms import RandomZoom, RandomHorizontalFlip\n",
    "\n",
    "dataset = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=False,\n",
    ")\n",
    "print(f\"Dataset: {len(dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Helper: show multi-view batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_views(batch, view_names, n=4, title=\"\"):\n",
    "    \"\"\"Display named views side-by-side.\n",
    "    \n",
    "    Handles both numpy HWC and torch CHW formats.\n",
    "    \"\"\"\n",
    "    num_views = len(view_names)\n",
    "    fig, axes = plt.subplots(n, num_views, figsize=(3 * num_views, 3 * n))\n",
    "    if n == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    \n",
    "    for col, name in enumerate(view_names):\n",
    "        imgs = batch[name]\n",
    "        for row in range(n):\n",
    "            img = imgs[row]\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                if img.ndim == 3 and img.shape[0] in (1, 3):\n",
    "                    img = img.permute(1, 2, 0)  # CHW -> HWC\n",
    "                img = img.cpu().float()\n",
    "                # Undo normalization if values look normalized\n",
    "                if img.min() < 0:\n",
    "                    mean = torch.tensor(IMAGENET_MEAN)\n",
    "                    std = torch.tensor(IMAGENET_STD)\n",
    "                    img = img * std + mean\n",
    "                img = img.clamp(0, 1).numpy()\n",
    "            else:\n",
    "                # numpy HWC uint8\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].axis('off')\n",
    "        axes[0, col].set_title(name, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1. Basic: identical copies (numpy)\n",
    "\n",
    "The simplest use: decode once, make named copies. Without `MultiCropPipeline`,\n",
    "the copies are identical numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SlipstreamLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    pipelines={'image': [\n",
    "        DecodeResizeCrop(resize_size=256, crop_size=224),\n",
    "        NamedCopies(['view1', 'view2']),\n",
    "    ]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "loader.shutdown()\n",
    "\n",
    "print(\"Batch keys:\", list(batch.keys()))\n",
    "print(f\"view1: {type(batch['view1']).__name__} {batch['view1'].shape}\")\n",
    "print(f\"view2: {type(batch['view2']).__name__} {batch['view2'].shape}\")\n",
    "print(f\"Identical: {np.array_equal(batch['view1'], batch['view2'])}\")\n",
    "\n",
    "show_views(batch, ['view1', 'view2'], n=4, title='Identical copies (numpy HWC)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Different zoom levels per view\n",
    "\n",
    "The main use case: decode once, then apply different `RandomZoom` transforms\n",
    "per view via `MultiCropPipeline`. View 1 is unzoomed, view 2 is zoomed to 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "loader = SlipstreamLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    pipelines={'image': [\n",
    "        DecodeResizeCrop(resize_size=256, crop_size=224),\n",
    "        NamedCopies(['original', 'zoomed_in']),\n",
    "        MultiCropPipeline({\n",
    "            'original': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(1.0, 1.0), x=0.5, y=0.5, device=DEVICE),\n",
    "            ],\n",
    "            'zoomed_in': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(0.5, 0.5), x=0.5, y=0.5, device=DEVICE),\n",
    "            ],\n",
    "        }),\n",
    "    ]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "loader.shutdown()\n",
    "\n",
    "print(f\"original:   {batch['original'].shape} {batch['original'].dtype}\")\n",
    "print(f\"zoomed_in: {batch['zoomed_in'].shape} {batch['zoomed_in'].dtype}\")\n",
    "\n",
    "show_views(batch, ['original', 'zoomed_in'], n=4,\n",
    "           title='Same crop, different zoom levels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Three views with graduated zoom\n",
    "\n",
    "You can create any number of named copies. Here we show three zoom levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SlipstreamLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    pipelines={'image': [\n",
    "        DecodeResizeCrop(resize_size=224, crop_size=224),\n",
    "        NamedCopies(['zoom_100', 'zoom_75', 'zoom_50']),\n",
    "        MultiCropPipeline({\n",
    "            'zoom_100': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(1.0, 1.0), x=0.5, y=0.5, device=DEVICE),\n",
    "            ],\n",
    "            'zoom_75': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(0.75, 0.75), x=0.5, y=0.5, device=DEVICE),\n",
    "            ],\n",
    "            'zoom_50': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(0.5, 0.5), x=0.5, y=0.5, device=DEVICE),\n",
    "            ],\n",
    "        }),\n",
    "    ]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "loader.shutdown()\n",
    "\n",
    "show_views(batch, ['zoom_100', 'zoom_75', 'zoom_50'], n=4,\n",
    "           title='Three zoom levels from one decode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 4. Augmented vs clean view\n",
    "\n",
    "Another common pattern: one view gets augmentations (flip, normalize),\n",
    "the other stays clean for visualization or as a reconstruction target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SlipstreamLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    pipelines={'image': [\n",
    "        DecodeResizeCrop(resize_size=256, crop_size=224),\n",
    "        NamedCopies(['clean', 'augmented']),\n",
    "        MultiCropPipeline({\n",
    "            'clean': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                # No augmentation — just convert to tensor\n",
    "            ],\n",
    "            'augmented': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomHorizontalFlip(p=1.0, device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(0.6, 0.6), x=0.5, y=0.5, device=DEVICE),\n",
    "            ],\n",
    "        }),\n",
    "    ]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "loader.shutdown()\n",
    "\n",
    "show_views(batch, ['clean', 'augmented'], n=4,\n",
    "           title='Clean vs augmented views')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Full training pipeline with normalization\n",
    "\n",
    "A realistic example: two normalized views ready for model input,\n",
    "with different zoom levels for multi-scale consistency training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SlipstreamLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    seed=43,\n",
    "    pipelines={'image': [\n",
    "        DecodeResizeCrop(resize_size=256, crop_size=224),\n",
    "        NamedCopies(['view1', 'view2']),\n",
    "        MultiCropPipeline({\n",
    "            'view1': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(1.0, 1.0), x=0.5, y=0.5, device=DEVICE),\n",
    "                Normalize(IMAGENET_MEAN, IMAGENET_STD, device=DEVICE),\n",
    "            ],\n",
    "            'view2': [\n",
    "                ToTorchImage(device=DEVICE),\n",
    "                RandomZoom(p=1.0, zoom=(0.5, 0.5), x=0.5, y=0.5, device=DEVICE),\n",
    "                Normalize(IMAGENET_MEAN, IMAGENET_STD, device=DEVICE),\n",
    "            ],\n",
    "        }),\n",
    "    ]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "loader.shutdown()\n",
    "\n",
    "print(f\"view1: {batch['view1'].shape}, dtype={batch['view1'].dtype}\")\n",
    "print(f\"  mean={batch['view1'].mean():.4f}, std={batch['view1'].std():.4f}\")\n",
    "print(f\"view2: {batch['view2'].shape}, dtype={batch['view2'].dtype}\")\n",
    "print(f\"  mean={batch['view2'].mean():.4f}, std={batch['view2'].std():.4f}\")\n",
    "print(f\"label: {batch['label'].shape}\")\n",
    "\n",
    "show_views(batch, ['view1', 'view2'], n=4,\n",
    "           title='Training-ready normalized views (zoom 1.0 vs 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 6. Verifying copy independence\n",
    "\n",
    "Each named copy is independent — modifying one does not affect the others.\n",
    "This is important because some transforms mutate tensors in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with numpy arrays\n",
    "nc = NamedCopies(['a', 'b', 'c'])\n",
    "original = np.random.rand(4, 224, 224, 3).astype(np.float32)\n",
    "copies = nc(original)\n",
    "\n",
    "# Mutate copy 'a'\n",
    "copies['a'][:] = 0.0\n",
    "\n",
    "print(\"After zeroing copy 'a':\")\n",
    "print(f\"  original unchanged: {original.mean():.4f} (should be ~0.5)\")\n",
    "print(f\"  copy 'a' zeroed:    {copies['a'].mean():.4f} (should be 0.0)\")\n",
    "print(f\"  copy 'b' unchanged: {copies['b'].mean():.4f} (should be ~0.5)\")\n",
    "print(f\"  copy 'c' unchanged: {copies['c'].mean():.4f} (should be ~0.5)\")\n",
    "\n",
    "# Test with torch tensors\n",
    "t = torch.randn(4, 3, 224, 224)\n",
    "t_copies = nc(t)\n",
    "t_copies['a'].zero_()\n",
    "\n",
    "print(f\"\\nTorch tensor independence:\")\n",
    "print(f\"  original mean: {t.mean():.4f} (should be ~0.0)\")\n",
    "print(f\"  copy 'a' mean: {t_copies['a'].mean():.4f} (should be 0.0)\")\n",
    "print(f\"  copy 'b' mean: {t_copies['b'].mean():.4f} (should be ~0.0, not exactly 0.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Role |\n",
    "|-----------|------|\n",
    "| `DecodeResizeCrop` / `DecodeCenterCrop` | Single decode + crop |\n",
    "| `NamedCopies(['v1', 'v2', ...])` | Duplicate into named dict (deep copy) |\n",
    "| `MultiCropPipeline({...})` | Apply per-view transforms |\n",
    "\n",
    "**When to use `NamedCopies`** vs `DecodeMultiRandomResizedCrop`:\n",
    "\n",
    "| Scenario | Use |\n",
    "|----------|-----|\n",
    "| Same center crop, different downstream transforms | `NamedCopies` |\n",
    "| Different random crops per view (SSL) | `DecodeMultiRandomResizedCrop` |\n",
    "| Different crop sizes per view (global/local) | `DecodeMultiRandomResizedCrop` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

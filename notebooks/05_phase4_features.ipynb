{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Phase 4: Cache & Format Enhancements\n",
    "\n",
    "This notebook tests the three new Phase 4 features:\n",
    "\n",
    "1. **`compute_normalization_stats`** — per-channel RGB mean/std from a slip cache\n",
    "2. **YUV output mode** — `DecodeYUVFullRes` and `DecodeYUVPlanes` pipelines\n",
    "3. **`sync_s3_dataset`** — fast S3→local sync via s5cmd\n",
    "\n",
    "Prereqs: run `uv sync` to rebuild the C extension with new YUV kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LITDATA_VAL_PATH = \"s3://visionlab-datasets/imagenet1k/pre-processed/s256-l512-jpgbytes-q100-streaming/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import SlipstreamDataset\n",
    "\n",
    "dataset = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=False,\n",
    ")\n",
    "print(f\"Dataset: {len(dataset):,} samples\")\n",
    "print(f\"Cache path: {dataset.cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Normalization Stats\n",
    "\n",
    "Compute per-channel RGB mean and std using Welford's online algorithm.\n",
    "First test on a small subset, then compare JPEG vs YUV420 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import SlipstreamLoader, DecodeCenterCrop\n",
    "from slipstream.cache import OptimizedCache\n",
    "\n",
    "# Ensure slip cache exists (loader builds it on first use)\n",
    "loader = SlipstreamLoader(\n",
    "    dataset, batch_size=256,\n",
    "    pipelines={'image': [DecodeCenterCrop(224)]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=True,\n",
    ")\n",
    "cache = loader.cache\n",
    "print(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import compute_normalization_stats\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# First, sanity-check: manually compute stats on a small batch via PIL/torchvision\n",
    "from slipstream import SlipstreamDataset\n",
    "ds_pil = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=True,\n",
    "    to_pil=False,  # returns CHW tensor\n",
    ")\n",
    "\n",
    "# Compute reference stats on first 200 images using torchvision decode\n",
    "channel_sum = np.zeros(3, dtype=np.float64)\n",
    "channel_sum_sq = np.zeros(3, dtype=np.float64)\n",
    "pixel_count = 0\n",
    "for i in range(200):\n",
    "    img = ds_pil[i]['image']  # CHW uint8 tensor\n",
    "    pixels = img.permute(1, 2, 0).numpy().reshape(-1, 3).astype(np.float64) / 255.0\n",
    "    channel_sum += pixels.sum(axis=0)\n",
    "    channel_sum_sq += (pixels * pixels).sum(axis=0)\n",
    "    pixel_count += pixels.shape[0]\n",
    "\n",
    "ref_mean = channel_sum / pixel_count\n",
    "ref_std = np.sqrt(channel_sum_sq / pixel_count - ref_mean ** 2)\n",
    "print(f\"Reference (torchvision, 200 imgs): mean={tuple(ref_mean.round(4))}, std={tuple(ref_std.round(4))}\")\n",
    "\n",
    "# Now compute with slipstream on the same 200 images\n",
    "stats_200 = compute_normalization_stats(cache, image_format=\"jpeg\", num_samples=200)\n",
    "print(f\"Slipstream (200 imgs):             mean={stats_200['mean']}, std={stats_200['std']}\")\n",
    "\n",
    "# Full dataset\n",
    "stats = compute_normalization_stats(cache, image_format=\"jpeg\")\n",
    "print(f\"\\nSlipstream (full dataset): {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Compare against known ImageNet stats (computed from training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import IMAGENET_MEAN, IMAGENET_STD\n",
    "\n",
    "print(f\"ImageNet reference mean (train): {IMAGENET_MEAN}\")\n",
    "print(f\"ImageNet reference std  (train): {IMAGENET_STD}\")\n",
    "print()\n",
    "print(f\"Computed mean (val):  {stats['mean']}\")\n",
    "print(f\"Computed std  (val):  {stats['std']}\")\n",
    "print()\n",
    "mean_diff = np.abs(np.array(IMAGENET_MEAN) - np.array(stats['mean']))\n",
    "std_diff = np.abs(np.array(IMAGENET_STD) - np.array(stats['std']))\n",
    "print(f\"Mean abs diff (train vs val): {tuple(mean_diff.round(4))}\")\n",
    "print(f\"Std abs diff  (train vs val): {tuple(std_diff.round(4))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# YUV Normalization Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build YUV420 cache if needed (uses the existing JPEG cache)\n",
    "loader_yuv = SlipstreamLoader(\n",
    "    dataset, batch_size=256,\n",
    "    image_format=\"yuv420\",\n",
    "    pipelines={'image': [DecodeCenterCrop(224)]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "stats_yuv = compute_normalization_stats(cache, image_format=\"yuv420\")\n",
    "\n",
    "print(f\"\\nJPEG mean: {stats['mean']}\")\n",
    "print(f\"YUV  mean: {stats_yuv['mean']}\")\n",
    "print(f\"JPEG std:  {stats['std']}\")\n",
    "print(f\"YUV  std:  {stats_yuv['std']}\")\n",
    "\n",
    "import numpy as np\n",
    "mean_diff = np.abs(np.array(stats['mean']) - np.array(stats_yuv['mean']))\n",
    "std_diff = np.abs(np.array(stats['std']) - np.array(stats_yuv['std']))\n",
    "print(f\"\\nMean abs diff: {mean_diff} (expected < 0.01)\")\n",
    "print(f\"Std abs diff:  {std_diff} (expected < 0.01)\")\n",
    "\n",
    "loader_yuv.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2. YUV Output Mode\n",
    "\n",
    "Two new pipelines for emitting YUV instead of RGB:\n",
    "- **`DecodeYUVFullRes`**: nearest-neighbor upsample U/V → `[H, W, 3]` (Y, U, V)\n",
    "- **`DecodeYUVPlanes`**: raw planes → `(Y, U, V)` with Y at full res, U/V at half res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import SlipstreamDataset, SlipstreamLoader, DecodeYUVFullRes, DecodeYUVPlanes\n",
    "\n",
    "LITDATA_VAL_PATH = \"s3://visionlab-datasets/imagenet1k/pre-processed/s256-l512-jpgbytes-q100-streaming/val/\"\n",
    "\n",
    "dataset = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=False,\n",
    ")\n",
    "print(f\"Dataset: {len(dataset):,} samples\")\n",
    "print(f\"Cache path: {dataset.cache_path}\")\n",
    "\n",
    "# Full-res YUV output\n",
    "loader_yuv_full = SlipstreamLoader(\n",
    "    dataset, batch_size=8, shuffle=False,\n",
    "    image_format=\"yuv420\",\n",
    "    pipelines={'image': [DecodeYUVFullRes()]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader_yuv_full))\n",
    "yuv_images = batch['image']  # list of [H, W, 3] arrays\n",
    "print(f\"DecodeYUVFullRes: {len(yuv_images)} images\")\n",
    "print(f\"  Shape: {yuv_images[0].shape}, dtype: {yuv_images[0].dtype}\")\n",
    "print(f\"  Channels are (Y, U, V) — not RGB\")\n",
    "loader_yuv_full.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecodeYUVFullRes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "yuv_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw planes output\n",
    "loader_yuv_planes = SlipstreamLoader(\n",
    "    dataset, batch_size=8, shuffle=False,\n",
    "    image_format=\"yuv420\",\n",
    "    pipelines={'image': [DecodeYUVPlanes()]},\n",
    "    exclude_fields=['path'],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader_yuv_planes))\n",
    "planes = batch['image']  # list of (Y, U, V) tuples\n",
    "y, u, v = planes[0]\n",
    "print(f\"DecodeYUVPlanes: {len(planes)} images\")\n",
    "print(f\"  Y shape: {y.shape} (full resolution)\")\n",
    "print(f\"  U shape: {u.shape} (half resolution)\")\n",
    "print(f\"  V shape: {v.shape} (half resolution)\")\n",
    "loader_yuv_planes.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Visual comparison: RGB vs YUV channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from slipstream import DecodeRandomResizedCrop, DecodeCenterCrop, DecodeOnly\n",
    "\n",
    "# Get same image as RGB and as YUV full-res\n",
    "loader_rgb = SlipstreamLoader(\n",
    "    dataset, batch_size=1, shuffle=False,\n",
    "    image_format=\"yuv420\",\n",
    "    pipelines={'image': [DecodeOnly()]},\n",
    "    exclude_fields=['path'], verbose=False,\n",
    ")\n",
    "loader_yuv_vis = SlipstreamLoader(\n",
    "    dataset, batch_size=1, shuffle=False,\n",
    "    image_format=\"yuv420\",\n",
    "    pipelines={'image': [DecodeYUVFullRes()]},\n",
    "    exclude_fields=['path'], verbose=False,\n",
    ")\n",
    "\n",
    "rgb_batch = next(iter(loader_rgb))\n",
    "yuv_batch = next(iter(loader_yuv_vis))\n",
    "\n",
    "rgb_img = rgb_batch['image'][0]  # [H, W, 3]\n",
    "yuv_img = yuv_batch['image'][0]  # [H, W, 3] — variable size, not cropped\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "axes[0].imshow(rgb_img)\n",
    "axes[0].set_title('RGB (CenterCrop 224)')\n",
    "axes[1].imshow(yuv_img[:, :, 0], cmap='gray')\n",
    "axes[1].set_title('Y (luma)')\n",
    "axes[2].imshow(yuv_img[:, :, 1], cmap='gray')\n",
    "axes[2].set_title('U (chroma)')\n",
    "axes[3].imshow(yuv_img[:, :, 2], cmap='gray')\n",
    "axes[3].set_title('V (chroma)')\n",
    "axes[4].imshow(yuv_img)\n",
    "axes[4].set_title('YUV as-is (false color)')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.suptitle('RGB vs YUV channel visualization', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "loader_rgb.shutdown()\n",
    "loader_yuv_vis.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### YUV→RGB round-trip verification\n",
    "\n",
    "Manually convert YUV full-res back to RGB and compare against the direct RGB decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from slipstream.decoders.yuv420_decoder import YUV420NumbaBatchDecoder\n",
    "from slipstream.cache import load_yuv420_cache\n",
    "\n",
    "def yuv_to_rgb_bt601(yuv: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert full-res YUV (BT.601) to RGB.\"\"\"\n",
    "    y = yuv[:, :, 0].astype(np.float32)\n",
    "    u = yuv[:, :, 1].astype(np.float32) - 128.0\n",
    "    v = yuv[:, :, 2].astype(np.float32) - 128.0\n",
    "    r = np.clip(y + 1.402 * v, 0, 255).astype(np.uint8)\n",
    "    g = np.clip(y - 0.344136 * u - 0.714136 * v, 0, 255).astype(np.uint8)\n",
    "    b = np.clip(y + 1.772 * u, 0, 255).astype(np.uint8)\n",
    "    return np.stack([r, g, b], axis=-1)\n",
    "\n",
    "# Load raw YUV420 data and decode both ways with the same decoder\n",
    "storage = load_yuv420_cache(cache.cache_dir, \"image\")\n",
    "indices = np.arange(8, dtype=np.int64)\n",
    "batch_data = storage.load_batch(indices)\n",
    "\n",
    "decoder = YUV420NumbaBatchDecoder(num_threads=4)\n",
    "\n",
    "rgb_images = decoder.decode_batch(\n",
    "    batch_data['data'], batch_data['sizes'],\n",
    "    batch_data['heights'], batch_data['widths'],\n",
    ")\n",
    "yuv_images = decoder.decode_batch_yuv_fullres(\n",
    "    batch_data['data'], batch_data['sizes'],\n",
    "    batch_data['heights'], batch_data['widths'],\n",
    ")\n",
    "\n",
    "for i in range(len(rgb_images)):\n",
    "    rgb_direct = rgb_images[i]\n",
    "    rgb_roundtrip = yuv_to_rgb_bt601(yuv_images[i])\n",
    "    diff = np.abs(rgb_direct.astype(np.int16) - rgb_roundtrip.astype(np.int16))\n",
    "    print(f\"  Image {i}: shape={rgb_direct.shape}, max diff={diff.max()}, mean diff={diff.mean():.2f}\")\n",
    "\n",
    "print(\"\\nExpected: max diff ≤ 1 (fixed-point rounding), mean ≈ 0\")\n",
    "decoder.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 3. Fast S3 Sync\n",
    "\n",
    "The `sync_s3_dataset` utility uses `s5cmd sync` for fast parallel S3→local copies.\n",
    "Also available as `presync_s3=True` on `SlipstreamLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Check if s5cmd is available\n",
    "if shutil.which(\"s5cmd\"):\n",
    "    print(\"s5cmd found ✓\")\n",
    "else:\n",
    "    print(\"s5cmd not found — install with: brew install peak/tap/s5cmd\")\n",
    "    print(\"Skipping S3 sync test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import sync_s3_dataset\n",
    "from slipstream.s3_sync import _deterministic_local_dir\n",
    "from slipstream.dataset import get_default_cache_dir\n",
    "from pathlib import Path\n",
    "\n",
    "# Show where data would be synced to (deterministic path)\n",
    "local_dir = _deterministic_local_dir(LITDATA_VAL_PATH, get_default_cache_dir())\n",
    "print(f\"Sync target: {local_dir}\")\n",
    "print(f\"Exists: {local_dir.exists()}\")\n",
    "if local_dir.exists():\n",
    "    n_files = sum(1 for _ in local_dir.iterdir() if _.is_file())\n",
    "    print(f\"Files already present: {n_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to actually run the sync (downloads ~7GB for ImageNet val)\n",
    "local_path = sync_s3_dataset(LITDATA_VAL_PATH)\n",
    "print(f\"Synced to: {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: use presync_s3 param on the loader\n",
    "# loader = SlipstreamLoader(\n",
    "#     dataset, batch_size=256,\n",
    "#     presync_s3=True,\n",
    "#     pipelines={'image': [DecodeRandomResizedCrop(224)]},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

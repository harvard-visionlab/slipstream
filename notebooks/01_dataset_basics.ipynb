{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SlipstreamDataset Basics\n",
    "\n",
    "This notebook demonstrates the basic usage of `SlipstreamDataset` for loading streaming datasets.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Intuitive API**: Use `remote_dir` and `cache_dir` instead of `Dir(...)`\n",
    "- **Automatic field detection**: Identifies image fields automatically\n",
    "- **Flexible decoding**: Optional automatic image decoding\n",
    "- **Pipeline support**: Per-field transforms for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset path (ImageNet validation, streaming format)\n",
    "LITDATA_VAL_PATH = \"s3://visionlab-datasets/imagenet1k/pre-processed/s256-l512-jpgbytes-q100-streaming/val/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Usage: Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import SlipstreamDataset\n",
    "\n",
    "# Create dataset with automatic decoding\n",
    "dataset = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=True,\n",
    "    to_pil=True,\n",
    ")\n",
    "\n",
    "# Show dataset info\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample\n",
    "sample = dataset[0]\n",
    "print(f\"Sample keys: {list(sample.keys())}\")\n",
    "print(f\"Image type: {type(sample['image'])}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "sample['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Bytes Mode (for high-performance training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset WITHOUT automatic decoding\n",
    "# This is what you'd use with SlipstreamLoader for training\n",
    "dataset_raw = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=False,\n",
    ")\n",
    "\n",
    "dataset_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw sample\n",
    "sample_raw = dataset_raw[0]\n",
    "print(f\"Image type: {type(sample_raw['image'])}\")\n",
    "print(f\"Image size: {len(sample_raw['image'])} bytes\")\n",
    "print(f\"First 16 bytes (JPEG header): {sample_raw['image'][:16].hex()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual decoding (what the loader will do)\n",
    "from slipstream import decode_image\n",
    "\n",
    "image_tensor = decode_image(sample_raw['image'], to_pil=False)\n",
    "print(f\"Decoded tensor shape: {image_tensor.shape}\")\n",
    "print(f\"Decoded tensor dtype: {image_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using with DataLoaders (variable-sized images)\n",
    "\n",
    "ImageNet images have varying sizes (256x384, 256x376, etc.), so we can't use `torch.stack`. \n",
    "We need a custom collate function that keeps images as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slipstream import SlipstreamDataset, list_collate_fn\n",
    "import torch\n",
    "\n",
    "# Dataset with tensor output\n",
    "dataset_tensor = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=True,\n",
    "    to_pil=False,  # Return tensors instead of PIL\n",
    ")\n",
    "\n",
    "# StreamingDataLoader with custom collate\n",
    "loader = StreamingDataLoader(\n",
    "    dataset_tensor,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=list_collate_fn,\n",
    ")\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(loader))\n",
    "print(f\"Batch keys: {list(batch.keys())}\")\n",
    "print(f\"Images: list of {len(batch['image'])} tensors\")\n",
    "print(f\"  First image shape: {batch['image'][0].shape}\")\n",
    "print(f\"  Second image shape: {batch['image'][1].shape}\")\n",
    "print(f\"Label: {batch['label'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The same collate_fn works with standard PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_pytorch = DataLoader(\n",
    "    dataset_tensor,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=list_collate_fn,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader_pytorch))\n",
    "print(f\"PyTorch DataLoader also works with list_collate_fn:\")\n",
    "print(f\"  Images: list of {len(batch['image'])} tensors\")\n",
    "print(f\"  Label: {batch['label'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Pipelines for Uniform Sizes (enables torch.stack)\n",
    "\n",
    "For training, you typically want stacked tensors `[B, C, H, W]`. Using `CenterCrop` or `RandomResizedCrop` in pipelines ensures all images are the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a pipeline that produces uniform 224x224 images\n",
    "image_pipeline = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: decode_image(x, to_pil=True)),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create dataset with pipeline\n",
    "dataset_pipeline = SlipstreamDataset(\n",
    "    remote_dir=LITDATA_VAL_PATH,\n",
    "    decode_images=False,  # Pipeline handles decoding\n",
    "    pipelines={'image': image_pipeline},\n",
    ")\n",
    "\n",
    "dataset_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a processed sample\n",
    "sample_processed = dataset_pipeline[0]\n",
    "print(f\"Processed image shape: {sample_processed['image'].shape}\")\n",
    "print(f\"Processed image dtype: {sample_processed['image'].dtype}\")\n",
    "print(f\"Processed image range: [{sample_processed['image'].min():.3f}, {sample_processed['image'].max():.3f}]\")\n",
    "\n",
    "# Now PyTorch DataLoader works because all images are 224x224!\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([sample['image'] for sample in batch])\n",
    "    labels = torch.tensor([sample['label'] for sample in batch])\n",
    "    return {'image': images, 'label': labels}\n",
    "\n",
    "loader_pytorch = DataLoader(\n",
    "    dataset_pipeline,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader_pytorch))\n",
    "print(f\"\\nPyTorch DataLoader batch:\")\n",
    "print(f\"  Image shape: {batch['image'].shape}\")  # [16, 3, 224, 224]\n",
    "print(f\"  Label shape: {batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**SlipstreamDataset** provides:\n",
    "- **Intuitive API**: `remote_dir` and `cache_dir` instead of `Dir(...)`\n",
    "- **Automatic field detection**: Identifies image fields automatically\n",
    "- **Pipeline support**: Per-field transforms for training\n",
    "- **LitData caching**: Automatic cache management under `~/.lightning/`\n",
    "\n",
    "**Key patterns**:\n",
    "- `decode_images=True` for interactive exploration (PIL/tensor output)\n",
    "- `decode_images=False` with `pipelines` for training (custom transforms)\n",
    "- Use `CenterCrop`/`RandomResizedCrop` in pipelines to enable `torch.stack`\n",
    "\n",
    "**Next**: See `02_loader_benchmarks.ipynb` for high-performance batch loading with `SlipstreamLoader`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
